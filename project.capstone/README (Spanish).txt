Bueno, iniciamos con el proyecto para el capstone.
El proyecto se realizara con las paginas https://datausa.io/ y https://data.gov/, y estara relacionado al mundo de la construccion.

La idea es realizarlo con ambas, ya que [la primera](https://datausa.io/) cuenta con la informacion mas limpia para extraer con API, por otro lado [la segunda](https://data.gov/) cuenta con informacion menos procesada, y si bien aparentemente es posible extraer la informacion de ciertos documentos estadisticos por API, se utilizara "Web Scrapping" para poder entrenar estas habilidades tambien.

¬øQu√© es Data USA?:
Es un proyecto del MIT Media Lab y Deloitte.
Integra datos del censo de EE. UU., Bureau of Labor Statistics, Bureau of Economic Analysis, etc.
Todo est√° presentado con visualizaciones interactivas y APIs accesibles.
Tiene datos sobre:
    Demograf√≠a (edad, sexo, origen).
    Educaci√≥n.
    Ingresos, pobreza, empleo.
    Vivienda, econom√≠a, transporte.
    Salud y seguros m√©dicos.

Ventajas para tu capstone:
F√°cil de usar ‚Üí La API est√° bien documentada, y tambi√©n pod√©s descargar CSV.
Datos limpios y estructurados ‚Üí No ten√©s que lidiar con formatos raros.
Visualizaci√≥n integrada ‚Üí Te sirve de inspiraci√≥n para tus propios gr√°ficos.
Relevancia con HomeScan ‚Üí
    Pod√©s analizar c√≥mo se relacionan ingresos y condiciones de vivienda.
    Estudiar costos de energ√≠a y pobreza energ√©tica.
    Cruzar datos de salud con condiciones habitacionales.
    Comparar zonas geogr√°ficas (estados/condados) y ver patrones.


Flujo correcto (adaptado al capstone)

Retrieving (API o Web Scraping)
    Obtener los datos:
        Si hay API ‚Üí urllib.request o requests + json.loads.
        Si es web scraping ‚Üí urllib + BeautifulSoup.
        Si es archivo CSV ‚Üí urllib + csv.reader.

Parsing / Processing
    Convertir lo que recib√≠s a estructuras de Python.
    Limpiar datos: strings ‚Üí n√∫meros, fechas, campos faltantes.
    Filtrar lo que te importa (ej: quedarte con Year, Population).

Database (Storing)
    Guardar los datos estructurados en SQLite.
    Tablas normalizadas, con claves, relaciones simples.
    Ejemplo: raw_population(year, nation, population).

Visualization
    Exportar datos procesados a un JSON/JS.
    Crear HTML con < script > que lea esos datos y pinte un gr√°fico simple (l√≠nea, barras, timeline, etc.).
    Igual que lo hac√©s en los ejemplos de m√≥dulos 6 del curso.

DATO FUNDAMENTAL: Para interiorizar a fondo todos los conocimientos, se identificara que conceptos del curso completo se estarian utilizando en el capstone, en que profundidad, y cuales no.

Conceptos de PY4E
    Variables, condicionales, loops
    Funciones
    Strings (b√∫squeda, slicing, regex)
    Files (leer, escribir)
    Lists / Tuples / Dictionaries
    Networking (sockets, urllib)
    Web scraping (BeautifulSoup)
    JSON / XML
    Databases (sqlite3)
    Visualizaci√≥n b√°sica en HTML/JS


Variables / loops / funciones

üü¢ Data USA: S√≠, se usan para recorrer resultados del JSON y estructurar datos.
üü¢ data.gov: S√≠, incluso m√°s, con bucles complejos para limpiar CSV/XML.

Strings

üü° Data USA: Poco uso, solo para formatear outputs.
üü¢ data.gov: Mucho uso: limpiar headers, parsear texto raro en CSV, trabajar con fechas.

Regex

üî¥ Data USA: No lo necesitar√≠as (datos limpios).
üü¢ data.gov: Muy √∫til para validar emails, direcciones o n√∫meros.

Files

üü¢ Data USA: Generar JSON o JS para la visualizaci√≥n final.
üü¢ data.gov: Igual, pero adem√°s pod√©s guardar CSV intermedios.

Lists / Tuples / Dicts

üü¢ Data USA: Uso directo (JSON ‚Üí dict/list ‚Üí loops).
üü¢ data.gov: Tambi√©n, y estructuras m√°s variadas si combin√°s tablas distintas.

Networking (APIs)

üü¢ Data USA: S√≠, con urllib.request para la API JSON.
üü° data.gov: A veces s√≠, a veces solo descarga manual (menos networking).

Web scraping (BeautifulSoup)

üî¥ Data USA: No hace falta.
üü° data.gov: Puede ser necesario si un dataset solo est√° en HTML.

JSON

üü¢ Data USA: Central, se usa mucho.
üü° data.gov: Presente en algunos, pero otros son CSV/XLS/XML.

XML

üî¥ Data USA: No aparece.
üü° data.gov: S√≠, hay datasets que todav√≠a usan XML.

Databases (SQLite)

üü¢ Data USA: Sencillo, una tabla con campos claros.
üü¢ data.gov: M√°s completo, varias tablas y joins.

Visualizaci√≥n en HTML/JS

üü¢ Data USA: R√°pido, export√°s un JSON limpio y grafic√°s.
üü° data.gov: Tambi√©n, pero requiere m√°s limpieza previa antes de graficar.

üëâ Con este formato, lo pod√©s leer r√°pido como un sem√°foro:

üü¢ = vas a practicarlo s√≠ o s√≠
üü° = depende del dataset, m√°s esfuerzo
üî¥ = casi no lo vas a tocar


TAMBIEN SE ANALIZARA A FONO LAS LINEAS DE CODIGO DE LO PRESENTADO POR CHARLES SEVERANCE EN EL CURSO. ESPECIFICAMENTE (gmane.py, gword.py, gbasic, gyear.py, gline.py) y tambien se analizara el funcionamiento del .htm / .html

------------------------------------------------------------------------------------------------------------

DESCRIPCI√ìN DETALLADA ‚Äì CAPSTONE ONE (DataUSA)

Este primer proyecto se desarroll√≥ sobre la base de **DataUSA**, una API bien documentada y con datos limpios, ideal para consolidar el flujo completo del curso: extracci√≥n, almacenamiento, procesamiento y visualizaci√≥n de informaci√≥n.

üîπ **Objetivo del proyecto**  
Analizar la evoluci√≥n del empleo total en los Estados Unidos y compararlo con el empleo en el sector inmobiliario (*Real State*). La idea es observar c√≥mo se comporta esta industria frente al promedio nacional, dado su v√≠nculo directo con la construcci√≥n.

---

### 1. Retrieving (Extracci√≥n de datos)
- El script `retrieving.py` conecta a la **API oficial de DataUSA**, que devuelve la informaci√≥n en formato JSON.
- Se realizaron dos consultas espec√≠ficas:
  1. **National** ‚Üí todos los empleos de la econom√≠a en general.  
  2. **Real State** ‚Üí empleos dentro del sector inmobiliario.  
- La funci√≥n `get_api()` gestiona:
  - Conexi√≥n con la URL.  
  - Lectura de la respuesta.  
  - Conversi√≥n del texto a estructuras Python con `json.loads()`.  

Con esto se puso en pr√°ctica:
- Uso de `urllib.request`.  
- Manejo de excepciones.  
- Comprensi√≥n de c√≥mo navegar un JSON con diccionarios y listas.  

---

### 2. Storing (Almacenamiento en SQLite)
- Los datos obtenidos se guardaron en `database.sqlite`.  
- Se cre√≥ la tabla `Realstate_Jobs` con tres columnas:
  - **Year** (a√±o).  
  - **Industry_Jobs** (cantidad de empleos).  
  - **Source** (origen: ‚ÄúNational‚Äù o ‚ÄúReal State‚Äù).  
- Gracias a este dise√±o, fue posible comparar ambas series en una misma estructura.  

---

### 3. Processing (Transformaci√≥n de datos)
- Con el script `graficgen.py`, los datos fueron recuperados de la base y transformados en un **diccionario de diccionarios** organizado por a√±o.  
- Luego, se gener√≥ el archivo `capstone.js` con un array en formato compatible con Google Charts, por ejemplo:  

```javascript
gline = [ ['a√±o', 'National', 'Real State'],
['2006', 1437174, 1498900],
['2008', 1151898, 1485000],
...
];
Este paso implic√≥ trabajar con:

Ciclos for para recorrer y agrupar datos.

Diccionarios anidados para estructurar la informaci√≥n.

Escritura en archivos con .write().

### 4. Visualization (Visualizaci√≥n con Google Charts)
- Finalmente, se carg√≥ capstone.js en un HTML para graficar los datos con Google Charts.
- Se generaron diferentes tipos de gr√°ficos interactivos:
- LineChart ‚Üí tendencias de cada serie.
- ComboChart ‚Üí barras + l√≠nea, comparando ambas evoluciones.
- AreaChart y ColumnChart ‚Üí para experimentar con distintos enfoques de visualizaci√≥n.
- Estos gr√°ficos mostraron de forma clara:
- La evoluci√≥n general del empleo en comparaci√≥n con el inmobiliario.
- Diferencias en las curvas seg√∫n los a√±os.
- El valor de representar un mismo dataset con m√∫ltiples formatos.

### 5. Aprendizajes Clave
- C√≥mo trabajar con APIs limpias y bien documentadas, que simplifican la extracci√≥n.
- Importancia de almacenar los datos en una base de datos relacional para organizar y comparar.
- C√≥mo preparar los datos pensando en la visualizaci√≥n final: es decir, estructurarlos de acuerdo al gr√°fico que se quiere lograr.
- Aplicaci√≥n de casi todos los m√≥dulos de PY4E en un flujo integrado:
- Networking y JSON.
- Diccionarios, listas y loops.
- SQL b√°sico con SQLite.
- Escritura de archivos.
- Visualizaci√≥n con HTML/JS.


------------------------------------------------------------------------------------------------------------

DESCRIPCI√ìN DETALLADA ‚Äì CAPSTONE TWO (NYC Construction Incidents ‚Äì Data.gov / NYC Open Data)

El segundo proyecto se propuso un nivel de complejidad mayor, trabajando con un dataset **real, extenso y menos procesado**: los incidentes en obras de construcci√≥n registrados en la ciudad de Nueva York.  
Esto permiti√≥ entrenar habilidades de web scraping, consultas SQL avanzadas y generaci√≥n de m√∫ltiples visualizaciones comparativas.

üîπ **Objetivo del proyecto**  
Analizar la ocurrencia de incidentes de construcci√≥n en NYC, identificando:
- Distribuci√≥n por borough (Manhattan, Bronx, Brooklyn, Queens, Staten Island).  
- Evoluci√≥n temporal mensual de los incidentes.  
- Principales causas (tipos de incidentes) y su impacto en t√©rminos de lesiones e incluso fatalidades.  

---

### 1. Retrieving (Obtenci√≥n de datos)
- El script `spider.py` se conect√≥ a la API de **NYC Open Data** (`bf97-mjsy/rows.json`).
- Se descarg√≥ un dataset en formato JSON con miles de registros.
- Cada registro inclu√≠a:
  - Fecha del incidente.
  - Tipo de incidente.
  - N√∫mero de fatalidades y lesiones.
  - Ubicaci√≥n (borough).
  - Latitud y longitud.  

**Transformaci√≥n inicial**:  
- El script normaliz√≥ la informaci√≥n en tres tablas dentro de la base `Construction-Related_Incidents.sqlite`:
  - `Incidents_type` ‚Üí cat√°logo de tipos de incidentes.  
  - `Ubication` ‚Üí cat√°logo de ubicaciones (boroughs).  
  - `Construction_Incidents` ‚Üí hechos registrados, con claves for√°neas.  

---

### 2. Storing (Base de datos)
- La estructura en SQLite permiti√≥ establecer relaciones entre tablas mediante IDs.  
- Este paso fue clave para poder realizar consultas complejas con `JOIN` y `GROUP BY`, generando agregaciones como:
  - N√∫mero de incidentes por mes y borough.  
  - Total de lesiones/fatalidades por tipo de incidente.  
  - Comparaciones cruzadas entre ubicaciones y tipos.  

---

### 3. Processing (Transformaci√≥n de datos)
Se crearon distintos scripts para agrupar y preparar la informaci√≥n:

- **`generator-colchart1.py`**  
  Agrupaci√≥n de **Fatalities e Injuries por borough** ‚Üí exporta `colchart.js`.  

- **`generator-colchart2.copy.py`**  
  Serie temporal mensual de incidentes por borough ‚Üí exporta `colchart2.js`.  
  - Uso avanzado de SQL para agrupar con `strftime('%Y-%m', c.date)`.  
  - Diccionarios anidados en Python para organizar por fechas y barrios.  

- **`generator-donut.py`**  
  Proporciones de incidentes por tipo ‚Üí genera dos archivos:  
  - `donut.js` (Fatalities por tipo).  
  - `donut2.js` (Injuries por tipo).  

---

### 4. Visualization (Visualizaci√≥n con Google Charts)
A partir de los archivos JS generados, se realizaron gr√°ficos interactivos:

1. **ColumnChart simple**: comparaci√≥n de Fatalities vs Injuries por borough.  
   - Brooklyn y Manhattan con la mayor cantidad de incidentes.  

2. **ColumnChart complejo (time series)**: evoluci√≥n mensual de incidentes (2024‚Äì2025).  
   - Brooklyn y Bronx con m√°s recurrencia a lo largo del tiempo.  

3. **Donut Chart ‚Äì Injuries**:  
   - M√°s del 50% de las lesiones provienen de *Worker Fell (ca√≠das de trabajadores)*.  

4. **Donut Chart ‚Äì Fatalities**:  
   - M√°s del 55% de las muertes tambi√©n por *Worker Fell*.  
   - Los dem√°s tipos de incidentes quedan muy por detr√°s en proporci√≥n.  

---

### 5. Aprendizajes Clave
- Extracci√≥n de datos **menos estructurados**: c√≥mo enfrentarse a datasets reales con ruido.  
- Normalizaci√≥n y modelado de datos en m√∫ltiples tablas SQLite.  
- Uso de **SQL avanzado** (`JOIN`, `GROUP BY`, `SUM`, `strftime` para fechas).  
- Manejo de diccionarios y estructuras anidadas en Python para agrupar resultados.  
- Creaci√≥n de distintas visualizaciones que muestran:
  - Comparaciones entre barrios.  
  - Evoluci√≥n en el tiempo.  
  - Distribuci√≥n porcentual de causas de incidentes.  

---

‚úÖ **En conclusi√≥n**:  
El **Capstone Two** lleva el aprendizaje un paso m√°s all√°: desde datos limpios a datos reales y complejos.  
Este proyecto demuestra c√≥mo **Python, SQLite y Google Charts** se pueden combinar para crear un an√°lisis completo y visualmente atractivo de un problema real, en este caso la seguridad en la construcci√≥n.
